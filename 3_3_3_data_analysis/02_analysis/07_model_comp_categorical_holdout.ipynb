{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_3_experience_sampling/3_3_3_data_analysis/02_analysis/helper_functions.py:23: DtypeWarning: Columns (45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../02_analysis/df_good.csv')\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import subject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to load all participants\n",
    "df_good = pd.read_csv('../02_analysis/df_good.csv')\n",
    "\n",
    "df = subject_df(df=df_good, sub_num=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the 3-day moving average with forward-looking window\n",
    "def forward_moving_average(series, window=3):\n",
    "    return series.rolling(window=window, min_periods=1).mean().shift(-window)\n",
    "\n",
    "# Apply the moving average calculation for each PID\n",
    "df_good['target'] = df_good.groupby('PID')['sr_gap_heuristic'].transform(lambda x: forward_moving_average(x, window=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 6, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0.5, 'classifier__reg_lambda': 0}\n",
      "Subject 1\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0.5, 'classifier__reg_lambda': 1}\n",
      "Subject 2\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 6, 'classifier__n_estimators': 200, 'classifier__reg_alpha': 0.1, 'classifier__reg_lambda': 0.5}\n",
      "Subject 3\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 200, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n",
      "Subject 4\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0.5}\n",
      "Subject 5\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0.5}\n",
      "Subject 6\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.2, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n",
      "Subject 7\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 6, 'classifier__n_estimators': 200, 'classifier__reg_alpha': 0.1, 'classifier__reg_lambda': 0.5}\n",
      "Subject 8\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.2, 'classifier__max_depth': 6, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0.5, 'classifier__reg_lambda': 0}\n",
      "Subject 9\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.2, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 1}\n",
      "Subject 10\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 200, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n",
      "Subject 11\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.2, 'classifier__max_depth': 3, 'classifier__n_estimators': 200, 'classifier__reg_alpha': 0.1, 'classifier__reg_lambda': 0.5}\n",
      "Subject 12\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n",
      "Subject 13\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 6, 'classifier__n_estimators': 200, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n",
      "Subject 14\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.2, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n",
      "Subject 15\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n",
      "Subject 16\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n",
      "Subject 17\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0.1, 'classifier__reg_lambda': 0.5}\n",
      "Subject 18\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0.1, 'classifier__reg_lambda': 0}\n",
      "Subject 19\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n",
      "Subject 20\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0.5}\n",
      "Subject 21\n",
      "Best parameters for XGBoost: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50, 'classifier__reg_alpha': 0, 'classifier__reg_lambda': 0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_test)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Number of correct predictions (assuming ensemble model predictions)\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxgb_accuracy_cv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m p_chance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Chance level accuracy\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Perform the binomial test\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['subject', 'logistic_reg_cv', 'random_forest_cv', 'xgboost_cv', 'ensemble_cv', 'logistic_reg_test', 'random_forest_test', 'xgboost_test', 'ensemble_test', 'p_val_xgb_cv', 'ffill', 'xgb_predictions'])\n",
    "\n",
    "# How many features to use\n",
    "num_features = 15\n",
    "\n",
    "for i in range(len(np.unique(df_good.PID))):\n",
    "    print(\"Subject\", i)\n",
    "    \n",
    "    try:\n",
    "        df_split = subject_df(df=df_good, sub_num=i)\n",
    "\n",
    "        # Use pd.qcut to create categories\n",
    "        df_split['y_cat'], bins = pd.qcut(df_split['target'], q=2, labels=['Low', 'High'], retbins=True)\n",
    "\n",
    "        # Make boolean\n",
    "        df_split['y_cat_high'] = df_split['y_cat'].cat.codes\n",
    "        df_split['y_cat_high'] = df_split['y_cat_high'].replace(-1, np.nan)\n",
    "\n",
    "        # Target column is next day's gap\n",
    "        df_split['y_cat_highNextDay'] = df_split['y_cat_high'].shift(-1)\n",
    "\n",
    "        # Drop non-boolean cat\n",
    "        df_split.drop(columns='y_cat', inplace=True)\n",
    "\n",
    "        # Remove trailing rows since no y1 value\n",
    "        df_split = df_split.iloc[:-2]\n",
    "\n",
    "        # Separate features and target variable\n",
    "        X = df_split.drop(columns=['y_cat_highNextDay'])\n",
    "        y = df_split['y_cat_highNextDay']\n",
    "\n",
    "        # Remove initial rows with NaN values for y_cat_high\n",
    "        while y.isnull().iloc[0]:\n",
    "            X = X.iloc[1:].reset_index(drop=True)\n",
    "            y = y.iloc[1:].reset_index(drop=True)\n",
    "        \n",
    "        # Remove all \"target\" columns\n",
    "        X = X.loc[:, ~X.columns.str.contains('target')]\n",
    "        \n",
    "        # Count missing values before imputation\n",
    "        missing_values_before = y.isnull().sum()\n",
    "\n",
    "        # Impute missing values in the target variable using forward fill\n",
    "        y = y.fillna(method='ffill')\n",
    "\n",
    "        # Count missing values after imputation\n",
    "        missing_values_after = y.isnull().sum()\n",
    "\n",
    "        # Calculate the number of imputed values\n",
    "        imputed_value_count = missing_values_before - missing_values_after\n",
    "\n",
    "        # Ensure there are no more NaN values in y\n",
    "        if y.isnull().sum() == 0:\n",
    "            # Train-test split for time series data\n",
    "            split_ratio = 0.8\n",
    "            split_index = int(len(X) * split_ratio)\n",
    "\n",
    "            X_train, X_test = X[:split_index], X[split_index:]\n",
    "            y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "            # Handle missing values and standardize features\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            # Feature selection\n",
    "            k_best = SelectKBest(score_func=f_classif, k=num_features)\n",
    "\n",
    "            # Models\n",
    "            log_reg = LogisticRegression(max_iter=1000)\n",
    "            rf = RandomForestClassifier(n_estimators=100)\n",
    "            xgb_clf = xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "            # TimeSeriesSplit\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "            # Pipelines\n",
    "            log_reg_pipeline = Pipeline([\n",
    "                ('imputer', imputer),\n",
    "                ('scaler', scaler),\n",
    "                ('k_best', k_best),\n",
    "                ('classifier', log_reg)\n",
    "            ])\n",
    "\n",
    "            rf_pipeline = Pipeline([\n",
    "                ('imputer', imputer),\n",
    "                ('scaler', scaler),\n",
    "                ('k_best', k_best),\n",
    "                ('classifier', rf)\n",
    "            ])\n",
    "\n",
    "            xgb_pipeline = Pipeline([\n",
    "                ('imputer', imputer),\n",
    "                ('scaler', scaler),\n",
    "                ('k_best', k_best),\n",
    "                ('classifier', xgb_clf)\n",
    "            ])\n",
    "\n",
    "            # Define the parameter grids for each model\n",
    "            param_grid_log_reg = {\n",
    "                'classifier__C': [0.01, 0.1, 1, 10, 100]\n",
    "            }\n",
    "\n",
    "            param_grid_rf = {\n",
    "                'classifier__n_estimators': [50, 100, 200],\n",
    "                'classifier__max_depth': [None, 10, 20, 30]\n",
    "            }\n",
    "\n",
    "            param_grid_xgb = {\n",
    "                'classifier__n_estimators': [50, 100, 200],\n",
    "                'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "                'classifier__max_depth': [3, 6, 9],\n",
    "                'classifier__reg_alpha': [0, 0.1, 0.5],\n",
    "                'classifier__reg_lambda': [0, 0.5, 1]\n",
    "            }\n",
    "\n",
    "            # Setup GridSearchCV for each model\n",
    "            grid_log_reg = GridSearchCV(log_reg_pipeline, param_grid_log_reg, cv=tscv, scoring='accuracy')\n",
    "            grid_rf = GridSearchCV(rf_pipeline, param_grid_rf, cv=tscv, scoring='accuracy')\n",
    "            grid_xgb = GridSearchCV(xgb_pipeline, param_grid_xgb, cv=tscv, scoring='accuracy')\n",
    "\n",
    "            # Fit the models with cross-validation\n",
    "            grid_log_reg.fit(X_train, y_train)\n",
    "            grid_rf.fit(X_train, y_train)\n",
    "            grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "            # Print best parameters for the XGBoost model\n",
    "            print(\"Best parameters for XGBoost:\", grid_xgb.best_params_)\n",
    "\n",
    "            # Best estimators from cross-validation\n",
    "            best_log_reg = grid_log_reg.best_estimator_\n",
    "            best_rf = grid_rf.best_estimator_\n",
    "            best_xgb = grid_xgb.best_estimator_\n",
    "\n",
    "            # Ensemble with VotingClassifier\n",
    "            voting_clf = VotingClassifier(estimators=[\n",
    "                ('log_reg', best_log_reg),\n",
    "                ('rf', best_rf),\n",
    "                ('xgb', best_xgb)\n",
    "            ], voting='soft')\n",
    "\n",
    "            # Fit the ensemble model\n",
    "            voting_clf.fit(X_train, y_train)\n",
    "\n",
    "            # Cross-validation scores for ensemble model\n",
    "            ensemble_cv_scores = cross_val_score(voting_clf, X_train, y_train, cv=tscv, scoring='accuracy')\n",
    "\n",
    "            # Predictions and accuracy on the held-out test set\n",
    "            y_pred_log_reg = best_log_reg.predict(X_test)\n",
    "            y_pred_rf = best_rf.predict(X_test)\n",
    "            y_pred_xgb = best_xgb.predict(X_test)\n",
    "            y_pred_ensemble = voting_clf.predict(X_test)\n",
    "\n",
    "            log_reg_accuracy_test = accuracy_score(y_test, y_pred_log_reg)\n",
    "            rf_accuracy_test = accuracy_score(y_test, y_pred_rf)\n",
    "            xgb_accuracy_test = accuracy_score(y_test, y_pred_xgb)\n",
    "            ensemble_accuracy_test = accuracy_score(y_test, y_pred_ensemble)\n",
    "\n",
    "            # Cross-validation accuracies\n",
    "            log_reg_accuracy_cv = grid_log_reg.best_score_\n",
    "            rf_accuracy_cv = grid_rf.best_score_\n",
    "            xgb_accuracy_cv = grid_xgb.best_score_\n",
    "            ensemble_accuracy_cv = ensemble_cv_scores.mean()\n",
    "\n",
    "            # Store predictions and actual values in a dictionary for XGBoost\n",
    "            prediction_dict = [{'predicted_value': pred, 'actual_value': actual} for pred, actual in zip(y_pred_xgb, y_test)]\n",
    "\n",
    "            # Number of test samples\n",
    "            n = len(y_test)\n",
    "            # Number of correct predictions (assuming ensemble model predictions)\n",
    "            k = int(xgb_accuracy_cv * n)\n",
    "            p_chance = 0.5  # Chance level accuracy\n",
    "\n",
    "            # Perform the binomial test\n",
    "            p_value = binom_test(k, n, p_chance, alternative='greater')\n",
    "\n",
    "            # Save results to the DataFrame using pd.concat\n",
    "            new_row = pd.DataFrame({\n",
    "                'subject': [i],\n",
    "                'logistic_reg_cv': [log_reg_accuracy_cv],\n",
    "                'random_forest_cv': [rf_accuracy_cv],\n",
    "                'xgboost_cv': [xgb_accuracy_cv],\n",
    "                'ensemble_cv': [ensemble_accuracy_cv],\n",
    "                'logistic_reg_test': [log_reg_accuracy_test],\n",
    "                'random_forest_test': [rf_accuracy_test],\n",
    "                'xgboost_test': [xgb_accuracy_test],\n",
    "                'ensemble_test': [ensemble_accuracy_test],\n",
    "                'p_val_xgb_cv': [p_value],\n",
    "                'ffill': [imputed_value_count],\n",
    "                'xgb_predictions': [prediction_dict]\n",
    "            })\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Skipping subject {i} because the target variable y still contains NaN values after imputation.\")\n",
    "            continue\n",
    "    except KeyError as err:\n",
    "        print(f'Error with subject {i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
