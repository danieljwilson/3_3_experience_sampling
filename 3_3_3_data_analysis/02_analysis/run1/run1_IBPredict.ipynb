{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375d9d3-7d03-49b7-b108-600aeb7a204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb777dd-3c69-4d80-88cd-e3123a9f6702",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6f9ac-c09a-4ecd-8085-835a1f235cc9",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c47689-f7cf-49d4-8cb3-e209c0b9a693",
   "metadata": {},
   "source": [
    "Going with XGBoost Ensemble\n",
    "\n",
    "**The steps for building a decision tree are as follows:**\n",
    "- Start with all examples at the root node\n",
    "- Calculate information gain for splitting on all possible features, and pick the one with the highest information gain\n",
    "- Split dataset according to the selected feature, and create left and right branches of the tree\n",
    "- Keep repeating splitting process until stopping criteria is met\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33725664-c84e-4d6a-9976-ae52dac6f0b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'public_tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpublic_tests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'public_tests'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from public_tests import *\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a1693-7087-40d2-ba30-9e906deb3e36",
   "metadata": {},
   "source": [
    "## Decision Tree: `HistGradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131921d5-c650-447c-ab3b-602234ba574c",
   "metadata": {},
   "source": [
    "For tree-based models, the handling of numerical and categorical variables is\n",
    "simpler than for linear models:\n",
    "* we do **not need to scale the numerical features**\n",
    "* using an **ordinal encoding for the categorical variables** is fine even if\n",
    "  the encoding results in an arbitrary ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f84f5-6d09-4a24-b77b-bccae9c75bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                          unknown_value=-1)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', categorical_preprocessor, categorical_columns)],\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57a983-b5f8-49ab-8280-0db8ca9a109c",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238296d3-ab6a-427f-bd73-512bc6e46345",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb544a4c-b48a-4be1-90a6-45093fb66a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"<PATH>\")\n",
    "# drop the duplicated column\n",
    "df = df.drop(columns=\"<DUPLICATED>\")\n",
    "\n",
    "target_name = \"<TARGET COLUMN>\"\n",
    "target = df[target_name]\n",
    "\n",
    "data = df.drop(columns=[target_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4be3c0-ec38-4152-818c-956e95c4f798",
   "metadata": {},
   "source": [
    "### Separate Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d941f-bf96-4c5a-846c-124f116b2f83",
   "metadata": {},
   "source": [
    "First look at columns to make sure things are not misclassified..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e32e42-6f14-4dde-8998-85dd2efd5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e4967-071e-452c-b203-ad6a5d3bcf4a",
   "metadata": {},
   "source": [
    "### Setup `dtype` Specific Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8fe0a-830d-42e4-b439-d2660278b724",
   "metadata": {},
   "source": [
    "We first define the columns depending on their data type:\n",
    "\n",
    "* **one-hot encoding** will be applied to categorical columns. Besides, we\n",
    "  use `handle_unknown=\"ignore\"` to solve the potential issues due to rare\n",
    "  categories.\n",
    "* **numerical scaling** numerical features which will be standardized.\n",
    "\n",
    "First, create the preprocessors for the numerical and categorical components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295c0e7e-5912-444e-935f-27abf7ece217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb7266-b8dd-44b9-8f0c-a488a69938bd",
   "metadata": {},
   "source": [
    "Now, create the transformer and associate weach of these preprocessors with their respective data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d34d8a-4b0c-40b9-8a4b-c0a167ebc418",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categorical_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[1;32m      3\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[0;32m----> 4\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone-hot-encoder\u001b[39m\u001b[38;5;124m'\u001b[39m, categorical_preprocessor, \u001b[43mcategorical_columns\u001b[49m),\n\u001b[1;32m      5\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m, numerical_preprocessor, numerical_columns)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categorical_columns' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "    ('standard_scaler', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc58b7e-8019-435b-9c96-31a43cbfa833",
   "metadata": {},
   "source": [
    "A `ColumnTransformer` does the following:\n",
    "\n",
    "* It **splits the columns** of the original dataset based on the column names\n",
    "  or indices provided. We will obtain as many subsets as the number of\n",
    "  transformers passed into the `ColumnTransformer`.\n",
    "* It **transforms each subsets**. A specific transformer is applied to\n",
    "  each subset: it will internally call `fit_transform` or `transform`. The\n",
    "  output of this step is a set of transformed datasets.\n",
    "* It then **concatenates the transformed datasets** into a single dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd4b2a-8e06-48e9-a844-92f34f08cc6c",
   "metadata": {},
   "source": [
    "**Note** you can also feed piplelines into the `ColumnTransformer`, for example if you need to impute values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc78b7fc-c08b-4896-bda4-dccc42a022f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numeric_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m numeric_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m      7\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler(),\n\u001b[1;32m      8\u001b[0m     )])\n\u001b[1;32m     10\u001b[0m categorical_transformer \u001b[38;5;241m=\u001b[39m OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#this means missing values will be assigned a vector of 0s\u001b[39;00m\n\u001b[1;32m     12\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m---> 13\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m, numeric_transformer, \u001b[43mnumeric_features\u001b[49m),\n\u001b[1;32m     14\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m, categorical_transformer, categorical_features)\n\u001b[1;32m     15\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numeric_features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(),\n",
    "    )])\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore') #this means missing values will be assigned a vector of 0s\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc7906-1f2b-4715-b1b4-490e5e7d292f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57de5e09-1f62-4b17-a312-8e87eb2e4561",
   "metadata": {},
   "source": [
    "`ColumnTransformer` is like any other scikit-learn transformer. In particular it can be combined with a classifier\n",
    "in a `Pipeline`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def996bc-ef67-4093-8ab3-86c427cb3d14",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ae07f-01ca-4589-83d7-4b3ae3cfc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=500))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901544c-c537-4ab3-8856-86da56149ec6",
   "metadata": {},
   "source": [
    "Once this is set up follow the standard process:\n",
    "\n",
    "- the `fit` method is called to preprocess the data and then train the\n",
    "  classifier of the preprocessed data;\n",
    "- the `predict` method makes predictions on new data;\n",
    "- the `score` method is used to predict on the test data and compare the\n",
    "  predictions to the expected test labels to compute the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af130402-6917-41eb-871f-8adea40373d4",
   "metadata": {},
   "source": [
    "**Alt** method of creating pipeline\n",
    "> Not sure about the difference between `make_pipeline` and `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf31e0-4705-4889-a1e1-4dbda3db4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e1ffe-2509-4b54-b10b-97688409bb22",
   "metadata": {},
   "source": [
    "#### Visualize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69075dc4-a408-4189-a410-3cdcbd979056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37603c9-4dd5-4053-aa59-82f7dc7d6905",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454bbdc-3b65-4f5b-b128-306dc00e6375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb51bd9f-7aac-45cc-a009-c7ce47fd9b31",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03bbad-c0fe-488d-bfae-36972b4712bb",
   "metadata": {},
   "source": [
    "Cross validation is combining the fit, predict, and scoring steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb754494-7eff-4db7-9879-51e5feb3fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, data, target, cv=5)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95635584-6172-4700-b415-d34d91ad0a49",
   "metadata": {},
   "source": [
    "**Alt**\n",
    "\n",
    "Shuffle split randomly selects for test set membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a396ac-b3d9-4de6-a6d8-8be446ea30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=40, test_size=0.3, random_state=0)\n",
    "cv_results = cross_validate(\n",
    "    regressor, data, target, cv=cv, scoring=\"neg_mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647aec83-e47d-40f8-9ca1-a778d1244262",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f6505-8bf2-460b-873b-e0c6a7f88d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cv_results[\"test_score\"]\n",
    "print(\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} Â± {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec1ec7-6ffa-48fd-855a-cd2f172a7e94",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9e609-622e-4ede-9cd6-1d1ea6a5e8da",
   "metadata": {},
   "source": [
    "# Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347f672-459c-4411-adbe-b57c477b8324",
   "metadata": {},
   "source": [
    "## Bias/Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825a7f5-71b7-4bc4-a1cd-61c38429a4ae",
   "metadata": {},
   "source": [
    "[Andrew Ng Video](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/L6SHx/diagnosing-bias-and-variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c8d6e-b4fa-4044-977d-f890b134b7fc",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e050bd5-ada1-4427-b2cd-ab65641193da",
   "metadata": {},
   "source": [
    "[Andrew Ng Video](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/FaPgS/error-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858097c6-7e57-44a0-833a-26ce12c5e2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
